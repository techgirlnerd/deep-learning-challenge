{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztGB7ugSxNBu",
        "outputId": "d51a91f7-bcd3-42bc-a623-8551883b1921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique values for APPLICATION_TYPE: 17\n",
            "T3     27037\n",
            "T4      1542\n",
            "T6      1216\n",
            "T5      1173\n",
            "T19     1065\n",
            "T8       737\n",
            "T7       725\n",
            "T10      528\n",
            "T9       156\n",
            "T13       66\n",
            "T12       27\n",
            "T2        16\n",
            "T25        3\n",
            "T14        3\n",
            "T29        2\n",
            "T15        2\n",
            "T17        1\n",
            "Name: APPLICATION_TYPE, dtype: int64\n",
            "\n",
            "Number of unique values for CLASSIFICATION: 71\n",
            "C1000    17326\n",
            "C2000     6074\n",
            "C1200     4837\n",
            "C3000     1918\n",
            "C2100     1883\n",
            "         ...  \n",
            "C4120        1\n",
            "C8210        1\n",
            "C2561        1\n",
            "C4500        1\n",
            "C2150        1\n",
            "Name: CLASSIFICATION, Length: 71, dtype: int64\n",
            "\n",
            "Number of unique values for ASK_AMT: 8747\n",
            "5000        25398\n",
            "10478           3\n",
            "15583           3\n",
            "63981           3\n",
            "6725            3\n",
            "            ...  \n",
            "5371754         1\n",
            "30060           1\n",
            "43091152        1\n",
            "18683           1\n",
            "36500179        1\n",
            "Name: ASK_AMT, Length: 8747, dtype: int64\n",
            "\n",
            "Unique values for APPLICATION_TYPE after binning:\n",
            "T3       27037\n",
            "T4        1542\n",
            "T6        1216\n",
            "T5        1173\n",
            "T19       1065\n",
            "T8         737\n",
            "T7         725\n",
            "T10        528\n",
            "T9         156\n",
            "Other      120\n",
            "Name: APPLICATION_TYPE, dtype: int64\n",
            "\n",
            "Unique values for CLASSIFICATION after binning:\n",
            "C1000    17326\n",
            "C2000     6074\n",
            "C1200     4837\n",
            "C3000     1918\n",
            "C2100     1883\n",
            "C7000      777\n",
            "Other      669\n",
            "C1700      287\n",
            "C4000      194\n",
            "C5000      116\n",
            "C1270      114\n",
            "C2700      104\n",
            "Name: CLASSIFICATION, dtype: int64\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 64)                3264      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3329 (13.00 KB)\n",
            "Trainable params: 3329 (13.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "804/804 [==============================] - 3s 3ms/step - loss: 0.5781 - accuracy: 0.7157 - val_loss: 0.5632 - val_accuracy: 0.7249\n",
            "Epoch 2/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5565 - accuracy: 0.7269 - val_loss: 0.5594 - val_accuracy: 0.7261\n",
            "Epoch 3/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5536 - accuracy: 0.7276 - val_loss: 0.5565 - val_accuracy: 0.7273\n",
            "Epoch 4/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5521 - accuracy: 0.7275 - val_loss: 0.5559 - val_accuracy: 0.7280\n",
            "Epoch 5/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5506 - accuracy: 0.7287 - val_loss: 0.5547 - val_accuracy: 0.7266\n",
            "Epoch 6/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5505 - accuracy: 0.7287 - val_loss: 0.5549 - val_accuracy: 0.7313\n",
            "Epoch 7/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5497 - accuracy: 0.7293 - val_loss: 0.5520 - val_accuracy: 0.7289\n",
            "Epoch 8/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5485 - accuracy: 0.7296 - val_loss: 0.5538 - val_accuracy: 0.7279\n",
            "Epoch 9/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5480 - accuracy: 0.7310 - val_loss: 0.5512 - val_accuracy: 0.7279\n",
            "Epoch 10/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5480 - accuracy: 0.7307 - val_loss: 0.5537 - val_accuracy: 0.7297\n",
            "Epoch 11/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5463 - accuracy: 0.7310 - val_loss: 0.5547 - val_accuracy: 0.7275\n",
            "Epoch 12/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5466 - accuracy: 0.7296 - val_loss: 0.5550 - val_accuracy: 0.7258\n",
            "Epoch 13/25\n",
            "804/804 [==============================] - 3s 3ms/step - loss: 0.5465 - accuracy: 0.7305 - val_loss: 0.5524 - val_accuracy: 0.7290\n",
            "Epoch 14/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5461 - accuracy: 0.7309 - val_loss: 0.5524 - val_accuracy: 0.7208\n",
            "Epoch 15/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5457 - accuracy: 0.7303 - val_loss: 0.5527 - val_accuracy: 0.7304\n",
            "Epoch 16/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5457 - accuracy: 0.7318 - val_loss: 0.5513 - val_accuracy: 0.7277\n",
            "Epoch 17/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5452 - accuracy: 0.7327 - val_loss: 0.5525 - val_accuracy: 0.7291\n",
            "Epoch 18/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5453 - accuracy: 0.7321 - val_loss: 0.5527 - val_accuracy: 0.7286\n",
            "Epoch 19/25\n",
            "804/804 [==============================] - 3s 3ms/step - loss: 0.5445 - accuracy: 0.7311 - val_loss: 0.5547 - val_accuracy: 0.7230\n",
            "Epoch 20/25\n",
            "804/804 [==============================] - 2s 2ms/step - loss: 0.5443 - accuracy: 0.7312 - val_loss: 0.5518 - val_accuracy: 0.7280\n",
            "Epoch 21/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5447 - accuracy: 0.7326 - val_loss: 0.5552 - val_accuracy: 0.7233\n",
            "Epoch 22/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5441 - accuracy: 0.7327 - val_loss: 0.5524 - val_accuracy: 0.7294\n",
            "Epoch 23/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5441 - accuracy: 0.7317 - val_loss: 0.5539 - val_accuracy: 0.7265\n",
            "Epoch 24/25\n",
            "804/804 [==============================] - 2s 3ms/step - loss: 0.5435 - accuracy: 0.7327 - val_loss: 0.5512 - val_accuracy: 0.7305\n",
            "Epoch 25/25\n",
            "804/804 [==============================] - 3s 3ms/step - loss: 0.5431 - accuracy: 0.7327 - val_loss: 0.5506 - val_accuracy: 0.7310\n",
            "268/268 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7310\n",
            "Loss: 0.5506112575531006\n",
            "Accuracy: 0.7309620976448059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Dependency importing\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Step 1: Reading the data\n",
        "url = \"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Identifying the target variable(s) and feature(s)\n",
        "target = \"IS_SUCCESSFUL\"\n",
        "features = df.drop(columns=[\"EIN\", \"NAME\", target])\n",
        "\n",
        "# Step 2: Dropping the EIN and NAME columns\n",
        "df = df.drop(columns=[\"EIN\", \"NAME\"])\n",
        "\n",
        "# Step 3: Determining the number of unique values for each column\n",
        "unique_counts = df.nunique()\n",
        "\n",
        "# Step 4: Determining the number of data points for each unique value in columns with > 10 unique values\n",
        "for column in df.columns:\n",
        "    if unique_counts[column] > 10:\n",
        "        print(f\"Number of unique values for {column}: {unique_counts[column]}\")\n",
        "        print(df[column].value_counts())\n",
        "        print()\n",
        "\n",
        "# Step 5: Binning \"rare\" categorical variables\n",
        "cutoff_point = 100\n",
        "\n",
        "# Identify categorical columns with more than 10 unique values\n",
        "categorical_columns = [column for column in df.columns if df[column].nunique() > 10 and df[column].dtype == 'object']\n",
        "\n",
        "# Iterate over each categorical column to bin rare values\n",
        "for column in categorical_columns:\n",
        "    # Determining the count of each unique value\n",
        "    value_counts = df[column].value_counts()\n",
        "\n",
        "    # Identifying values with fewer than the cutoff point data points\n",
        "    rare_values = value_counts[value_counts < cutoff_point].index.tolist()\n",
        "\n",
        "    # Replacing rare values with \"Other\"\n",
        "    df[column] = df[column].apply(lambda x: \"Other\" if x in rare_values else x)\n",
        "\n",
        "# Verifying if binning was successful\n",
        "for column in categorical_columns:\n",
        "    print(f\"Unique values for {column} after binning:\")\n",
        "    print(df[column].value_counts())\n",
        "    print()\n",
        "\n",
        "# Step 6: Encoding the categorical variables\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "# Step 7: Split the data into features and target arrays, then split into training and testing datasets\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Step 8: Scaling the training and testing features datasets\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fitting the scaler to the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 9: Determining the number of input features\n",
        "input_features = X_train_scaled.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Creating a neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Step 2: Adding the first hidden layer with appropriate activation function\n",
        "model.add(Dense(units=64, activation='relu', input_dim=input_features))\n",
        "\n",
        "# Step 3: Creating an output layer with an appropriate activation function\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Step 3: Checking the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# Step 5: Compiling and training the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 6: Creating a callback to save the model's weights every five epochs\n",
        "checkpoint_path = \"model_checkpoint/checkpoint\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_freq='epoch',\n",
        "                                      period=5)\n",
        "\n",
        "# Step 7: Training the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=25, callbacks=[checkpoint_callback], validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Step 8: Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Step 9: Saving the model to an HDF5 file\n",
        "model.save(\"AlphabetSoupCharity.h5\")\n",
        "\n",
        "#Optimization attempts in the AlphabetSoupCharity_Optimization.py file"
      ],
      "metadata": {
        "id": "nUmSARw33_B_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}